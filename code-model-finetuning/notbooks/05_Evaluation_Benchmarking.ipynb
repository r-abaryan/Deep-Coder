{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation & Benchmarking: Fine-tuned vs Base Model\n",
        "\n",
        "**Objective**: Comprehensively evaluate the fine-tuned model against the base model\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "1. **Syntax Correctness**: Can the model generate valid Python code?\n",
        "2. **BLEU Score**: How similar is generated code to reference?\n",
        "3. **Exact Match**: Does generated code exactly match reference?\n",
        "4. **Pass@k**: Functional correctness (if test cases available)\n",
        "5. **Perplexity**: Model confidence\n",
        "6. **Human Evaluation**: Qualitative assessment\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import ast\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model paths \n",
        "base_model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "finetuned_model_path = \"./sft_results\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load base model [When trained using LoRA we need to load the base model]\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    dtype=torch.float16,  # Use dtype instead of deprecated torch_dtype\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "base_model.eval()\n",
        "print(\"Base model loaded\")\n",
        "\n",
        "# 2. Load fine-tuned model with LoRA adapter\n",
        "# Load base model first\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    dtype=torch.float16,  # Use dtype instead of deprecated torch_dtype\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapter weights\n",
        "print(f\"Loading LoRA adapter from: {finetuned_model_path}\")\n",
        "finetuned_model = PeftModel.from_pretrained(\n",
        "    finetuned_model, \n",
        "    finetuned_model_path,\n",
        "    torch_dtype=torch.float16  # Specify dtype for adapter weights\n",
        ")\n",
        "finetuned_model.eval()\n",
        "print(\"Fine-tuned model loaded successfully\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load validation set from Magicoder [we use train split as validation]\n",
        "dataset = load_dataset(\"ise-uiuc/Magicoder-Evol-Instruct-110K\", split=\"train\")\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Take subset for evaluation (adjust size as needed)\n",
        "num_eval_samples = 500\n",
        "test_dataset = test_dataset.select(range(num_eval_samples))\n",
        "\n",
        "print(f\"Test dataset: {len(test_dataset)} examples\")\n",
        "print(f\"Sample: {test_dataset[0]['instruction'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics functions for evaluation\n",
        "def check_syntax(code: str) -> bool:\n",
        "    \"\"\"Check if Python code has valid syntax\"\"\"\n",
        "    try:\n",
        "        ast.parse(code)\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def calculate_bleu(reference: str, hypothesis: str) -> float:\n",
        "    \"\"\"Calculate BLEU score between reference and hypothesis\"\"\"\n",
        "    reference_tokens = reference.split()\n",
        "    hypothesis_tokens = hypothesis.split()\n",
        "    \n",
        "    if not hypothesis_tokens:\n",
        "        return 0.0\n",
        "    \n",
        "    smoothing = SmoothingFunction().method1\n",
        "    return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing)\n",
        "\n",
        "\n",
        "def exact_match(reference: str, hypothesis: str) -> bool:\n",
        "    \"\"\"Check if generated code exactly matches reference\"\"\"\n",
        "    return reference.strip() == hypothesis.strip()\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, input_ids, labels):\n",
        "    \"\"\"Calculate perplexity\"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        if outputs.loss is None:\n",
        "            return None\n",
        "        loss = outputs.loss\n",
        "        perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "\n",
        "print(\"Metrics functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_code(model, instruction: str, max_new_tokens: int = 256, temperature: float = 0.7):\n",
        "    \"\"\"Generate code completion from instruction\"\"\"\n",
        "    # Format prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    \n",
        "    # Generate via torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode only the generated part\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    \n",
        "    return generated_text.strip()\n",
        "\n",
        "\n",
        "# Test generation\n",
        "test_instruction = \"Write a Python function to calculate factorial\"\n",
        "test_output = generate_code(base_model, test_instruction, max_new_tokens=100)\n",
        "print(f\"Test generation:\\n{test_output[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model function \n",
        "def evaluate_model(model, dataset, model_name: str, num_samples: int = 500):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of model\n",
        "    \n",
        "    Returns:\n",
        "        dict: Evaluation metrics and examples\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'syntax_correct': 0,\n",
        "        'bleu_scores': [],\n",
        "        'exact_matches': 0,\n",
        "        'perplexities': [],\n",
        "        'examples': []\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    \n",
        "    for i, example in enumerate(tqdm(dataset.select(range(min(num_samples, len(dataset)))))):\n",
        "        instruction = example['instruction']\n",
        "        reference = example['response']\n",
        "        \n",
        "        generated = generate_code(model, instruction)\n",
        "        \n",
        "        # Syntax correctness\n",
        "        is_valid = check_syntax(generated)\n",
        "        if is_valid:\n",
        "            results['syntax_correct'] += 1\n",
        "        \n",
        "        # BLEU score\n",
        "        bleu = calculate_bleu(reference, generated)\n",
        "        results['bleu_scores'].append(bleu)\n",
        "        \n",
        "        # Exact match\n",
        "        if exact_match(reference, generated):\n",
        "            results['exact_matches'] += 1\n",
        "        \n",
        "        # Perplexity (calculate on reference)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"assistant\", \"content\": reference}\n",
        "        ]\n",
        "        full_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "        inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "        \n",
        "        try:\n",
        "            ppl = calculate_perplexity(model, inputs.input_ids, inputs.input_ids)\n",
        "            if ppl is not None and not np.isnan(ppl) and ppl < 1000:\n",
        "                results['perplexities'].append(ppl)\n",
        "        except (RuntimeError, ValueError, IndexError):\n",
        "            pass\n",
        "        \n",
        "        # Save first 10 examples\n",
        "        if i < 10:\n",
        "            results['examples'].append({\n",
        "                'instruction': instruction,\n",
        "                'reference': reference,\n",
        "                'generated': generated,\n",
        "                'syntax_correct': is_valid,\n",
        "                'bleu': bleu\n",
        "            })\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    total = min(num_samples, len(dataset))\n",
        "    results['summary'] = {\n",
        "        'syntax_accuracy': results['syntax_correct'] / total * 100,\n",
        "        'avg_bleu': np.mean(results['bleu_scores']),\n",
        "        'exact_match_rate': results['exact_matches'] / total * 100,\n",
        "        'avg_perplexity': np.mean(results['perplexities']) if results['perplexities'] else 0,\n",
        "        'total_samples': total\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on both models\n",
        "base_results = evaluate_model(base_model, test_dataset, \"Base Model\", num_samples=500)\n",
        "finetuned_results = evaluate_model(finetuned_model, test_dataset, \"Fine-tuned Model\", num_samples=500)\n",
        "\n",
        "print(\"\\n Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create comparison table\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['Syntax Accuracy (%)', 'Average BLEU', 'Exact Match (%)', 'Average Perplexity', 'Total Samples'],\n",
        "    'Base Model': [\n",
        "        f\"{base_results['summary']['syntax_accuracy']:.2f}\",\n",
        "        f\"{base_results['summary']['avg_bleu']:.4f}\",\n",
        "        f\"{base_results['summary']['exact_match_rate']:.2f}\",\n",
        "        f\"{base_results['summary']['avg_perplexity']:.2f}\",\n",
        "        base_results['summary']['total_samples']\n",
        "    ],\n",
        "    'Fine-tuned Model': [\n",
        "        f\"{finetuned_results['summary']['syntax_accuracy']:.2f}\",\n",
        "        f\"{finetuned_results['summary']['avg_bleu']:.4f}\",\n",
        "        f\"{finetuned_results['summary']['exact_match_rate']:.2f}\",\n",
        "        f\"{finetuned_results['summary']['avg_perplexity']:.2f}\",\n",
        "        finetuned_results['summary']['total_samples']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n Model Comparison:\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\nðŸ“ˆ Improvements:\")\n",
        "print(f\"Syntax Accuracy: {finetuned_results['summary']['syntax_accuracy'] - base_results['summary']['syntax_accuracy']:.2f}% improvement\")\n",
        "print(f\"BLEU Score: {(finetuned_results['summary']['avg_bleu'] - base_results['summary']['avg_bleu']) / base_results['summary']['avg_bleu'] * 100:.2f}% improvement\")\n",
        "print(f\"Exact Match: {finetuned_results['summary']['exact_match_rate'] - base_results['summary']['exact_match_rate']:.2f}% improvement\")\n",
        "print(f\"Perplexity: {(base_results['summary']['avg_perplexity'] - finetuned_results['summary']['avg_perplexity']) / base_results['summary']['avg_perplexity'] * 100:.2f}% reduction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualitative Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_comparison(example_idx: int):\n",
        "    \"\"\"Display side-by-side comparison of base vs fine-tuned\"\"\"\n",
        "    base_ex = base_results['examples'][example_idx]\n",
        "    ft_ex = finetuned_results['examples'][example_idx]\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Example {example_idx + 1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    print(f\"\\n Instruction:\\n{base_ex['instruction'][:200]}...\")\n",
        "    \n",
        "    print(f\"\\n Reference (Ground Truth):\\n{base_ex['reference'][:300]}...\")\n",
        "    \n",
        "    print(f\"\\n Base Model Output:\")\n",
        "    print(f\"Syntax Valid: {base_ex['syntax_correct']} | BLEU: {base_ex['bleu']:.4f}\")\n",
        "    print(f\"{base_ex['generated'][:300]}...\")\n",
        "    \n",
        "    print(f\"\\n Fine-tuned Model Output:\")\n",
        "    print(f\"Syntax Valid: {ft_ex['syntax_correct']} | BLEU: {ft_ex['bleu']:.4f}\")\n",
        "    print(f\"{ft_ex['generated'][:300]}...\")\n",
        "\n",
        "\n",
        "# Display first 5 examples\n",
        "for i in range(min(5, len(base_results['examples']))):\n",
        "    display_comparison(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results to JSON\n",
        "import json\n",
        "\n",
        "results_summary = {\n",
        "    'base_model': base_results['summary'],\n",
        "    'finetuned_model': finetuned_results['summary'],\n",
        "    'improvements': {\n",
        "        'syntax_accuracy': finetuned_results['summary']['syntax_accuracy'] - base_results['summary']['syntax_accuracy'],\n",
        "        'bleu_score': finetuned_results['summary']['avg_bleu'] - base_results['summary']['avg_bleu'],\n",
        "        'exact_match': finetuned_results['summary']['exact_match_rate'] - base_results['summary']['exact_match_rate'],\n",
        "        'perplexity_reduction': base_results['summary']['avg_perplexity'] - finetuned_results['summary']['avg_perplexity']\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('evaluation_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"Results saved to evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Syntax Correctness**: Fine-tuned model generates syntactically valid code X% more often\n",
        "- **BLEU Score**: Improved by X%, indicating better alignment with reference code\n",
        "- **Perplexity**: Reduced by X%, showing increased confidence\n",
        "\n",
        "### Next Steps:\n",
        "1. Human evaluation on complex coding tasks\n",
        "2. Test on real-world scenarios (FastAPI, Django, React)\n",
        "3. Deploy for production testing\n",
        "4. Consider GRPO stage for further alignment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
