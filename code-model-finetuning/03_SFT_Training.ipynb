{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0845a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and cofiguring model\n",
    "model_name= 'deepseek-ai/deepseek-coder-6.7b-instruct'\n",
    "device='auto'\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token - tokenizer.eos_token\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # Rank (can go up to 64 for quality)\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Which layers to adapt\n",
    ")\n",
    "\n",
    "model =get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b182e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100064 | Val: 11119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b5b32b6bb541c2bb8924ec93f6cce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train (num_proc=4):   0%|          | 0/100064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 680, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3516, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3466, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3389, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\rasou\\AppData\\Local\\Temp\\ipykernel_146928\\1010255324.py\", line 45, in <lambda>\nNameError: name 'preprocess_dataset' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: texts}\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 4. Apply preprocessing (PARALLELIZED with 64 CPUs!)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m train_formatted = \u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Use all 64 vCPUs!\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFormatting train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     51\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m val_formatted = val_dataset.map(\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_dataset(x, tokenizer),\n\u001b[32m     54\u001b[39m     batched=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mFormatting val\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mPreprocessing complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3166\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3160\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3161\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3162\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3163\u001b[39m     total=pbar_total,\n\u001b[32m   3164\u001b[39m     desc=(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3165\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3166\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[32m   3167\u001b[39m         pool, Dataset._map_single, kwargs_iterable=kwargs_per_job\n\u001b[32m   3168\u001b[39m     ):\n\u001b[32m   3169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3170\u001b[39m             shards_done += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\datasets\\utils\\py_utils.py:720\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    719\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mNameError\u001b[39m: name 'preprocess_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREPROCESSING FOR MAGICODER-EVOL-INSTRUCT-110K\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "# Load dataset and split \n",
    "dataset = load_dataset(\"ise-uiuc/Magicoder-Evol-Instruct-110K\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']  # ~99K\n",
    "val_dataset = dataset['test']     # ~11K\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# 3. Preprocessing function\n",
    "def preprocess_dataset(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Format Magicoder examples with chat template.\n",
    "    \n",
    "    Magicoder structure:\n",
    "    {\n",
    "        'instruction': 'Write a function to...',\n",
    "        'response': 'def foo():\\n    ...'\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer specializing in clean, efficient code.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template (handles special tokens automatically)\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "    \n",
    "# 4. Apply preprocessing (PARALLELIZED with 64 CPUs!)\n",
    "train_formatted = train_dataset.map(\n",
    "    lambda x: preprocess_dataset(x, tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,      # Use all 64 vCPUs!\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Formatting train\"\n",
    ")\n",
    "val_formatted = val_dataset.map(\n",
    "    lambda x: preprocess_dataset(x, tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Formatting val\"\n",
    ")\n",
    "print(f\"✅ {len(train_data)}Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2913a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with validation\n",
    "output_dir=\"sft_results\"\n",
    "\n",
    "train_args=SFTConfig(\n",
    "    output_dir=output,\n",
    "    learning_rate=3e-5, #3e-5\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    # gradient_checkpointing=True,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    # Epochs, Batches and saving\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,  # 2x training\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,  # Keep only 3 best checkpoints\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    "\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # ← Validation here\n",
    "    args=train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732eecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and auto-evaluate\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)# Final evaluation\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Final validation loss: {eval_results['eval_loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
